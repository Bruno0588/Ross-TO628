---
title: "HW3 Telemarketing:KNN"
author: "Mark Bruno"
date: "4/02/2022"
output:
  html_document:
    toc: yes
    theme: readable
    highlight: tango
    code_folding: show
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Downloading and loading the data

```{r}
#Open Data
tele <- read.csv("tele.csv", stringsAsFactors = TRUE)
summary(tele)

#We are deleting the "duration" variable because it is an after the fact measurement. We only should be using variables that we know before the call
tele$duration <- NULL

# Deleting the column X
tele$X <- NULL

# Changing pdays to a dummy and deleting pdays
tele$pdaysdummy <- ifelse(tele$pdays == 999, 0, 1)
tele$pdays <- NULL

str(tele)

```

## Getting Data Ready for Analysis

```{r}
# Using model.matrix to convert all the factors to dummy variables
# We are converting all of the factors into dummy variables as the input into KNN has to be numeric

telemm <- as.data.frame(model.matrix(~.-1,tele))
str(telemm)

# Randomize the rows in the data (shuffling the rows)
set.seed(12345)
tele_random <- telemm[sample(nrow(telemm)),]

# Normalize the data
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}

# We are going to normalize everything 
tele_norm <- as.data.frame(lapply(tele_random, normalize))

rm(tele)
rm(tele_random)
rm(telemm)
```


## Partitioning Train and Test Samples

```{r}
# Selects 10000 random rows for test data
set.seed(12345)
test_set <- sample(1:nrow(tele_norm), 0.2*nrow(tele_norm)) 
# Depending on R-version and computer, different rows may be selected. 
# If that happens, results are different. 

#moving columns 
library(dplyr)
tele_norm <- tele_norm %>% relocate(yyes, .after = last_col())

# Create a train set and test set
#First the predictors - all columns except the class column
tele_train <- tele_norm[-test_set, ]
tele_test <- tele_norm[test_set, ]

rm(tele_norm)
```

> Now you are ready to build your KNN model. Feel free to modify the data load, cleaning and preparation code above as per your preference.

## Using upsampling in the training data

```{r}

#Upsampling 
library("caret")
set.seed(12345)
tele_train1 <- upSample(tele_train[ ,-ncol(tele_train)], as.factor(tele_train$yyes))

summary(tele_train1)

#Renaming column 

tele_train1$yyes <- as.numeric(tele_train1$Class)
summary(tele_train1)
tele_train1$Class <- NULL
tele_train1$yyes <- tele_train1$yyes -1

summary(tele_train1)

#Creating labels for train and test 
#Label for test, essentially just the Y value 

train_label <- tele_train1$yyes
tele_train1$yyes <- NULL
table(train_label)

test_label <- tele_test$yyes
tele_test$yyes <- NULL

table(test_label)
```


## Build KNN Model

```{r}
library(class)
#KNN Command, outputs prediction for the test data

sqrt(nrow(tele_train1))

test_pred <- knn(train = tele_train1, test = tele_test, cl = train_label, k = 242)
summary(test_pred)

```

# Confusion Matrix 
```{r}


confusionMatrix(as.factor(test_label), as.factor(test_pred), positive = "1")

```

## Conclusion
The accuracy of the ANN is more accurate at 84.69% vs. the KNN accuracy at 82.18%. However, the KNN model has higher sensitivity at 35% vs ANN at 26.3% (Prediction of True Positives), and the KNN also has higher specificity at 94.6% vs. ANN at 91.9% (True Negatives). 

